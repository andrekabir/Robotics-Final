# Robotics Final

## Project Description
Based on various challenges we faced along the way, we had to repeatedly redefine our goals and the scope. In the end, our final project topic was to implement gesture recognition (computer vision algorithm) and an A* path following algorithm, which also included having the physical robot follow this path in the maze. This project idea was inspired by the idea of a robotic dog -- "turtledog" is an endearing term we've used to describe the final product -- which can recognize commands from a user that signal where it should travel, and it will travel to the destination using an optimal path. We can extend this project idea beyond the classroom by connecting it to a "search and retrieval" or even "search and rescue"-type situation. To accomplish our project task, we split our project into 3 primary components: 1) the A* algorithm, 2) the gesture recognition component, and 3) the robot movement itself, using Odometry to follow the A*-planned path. 
## Pictures of A* generated maps and video demos (gesture recognition & physical turtlebot following A* path in maze using Odom)
# Different types of A* paths generated, given different start and end points in the map:
![map1](https://user-images.githubusercontent.com/55162345/206757938-5491ee86-23f7-47cf-83f8-f58bceac4b59.png)
![map2](https://user-images.githubusercontent.com/55162345/206757962-474bd843-d25b-42ff-b3ce-2a18028305e1.png)
![map3](https://user-images.githubusercontent.com/55162345/206757989-13bebc3d-170f-4a44-b7c2-f9fb4ae3289b.png)
# Example of a reduced map used to move physical turtlebot
![map5](https://user-images.githubusercontent.com/55162345/206758026-ae179313-16d6-42de-a4fa-30a74abed1f9.png)
# Demo of gesture recognition
https://youtu.be/_OyogAWStVs
# Demo of turtlebot traveling along computed A* path
https://youtu.be/AeMjspqZgjY
## System Architecture
For moving the robot using the A* path, we do this in move_robot.py. At a high level, what is done is that the shortened A* path is taken into this function, giving us a shorter list of particles and the angle at which the need to go to get to the next point. Move robot takes this list of particles and gets the linear and angular change between two points, in the function get_linear_distance_array. Once we have this list of “instructions” for the robot, we use our odom_callback function to take in the odometry data. We track the previous odom position in old_odom, and calculate the distance between the current odom position and the previous one, and once we have travelled more than the required distance between the two points, as indicated by the linear distance array. Once this is done, the robot stops and turns at a fixed speed, with the time taken on this turn depending on how big the turn is. After this, the whole process repeats, until the movement is finished.

'path_planner.py' - This script takes in the map from RViz, a start point and end point. Then, it initializes a graph data structure from the map and runs A* to find an optimal path from start to end. Lines 62-96 describe a Cell - a unit on which A* is performed. Lines 97 - 265 define the CellGraph class - which holds a two-dimensional array of cells, each representing an empty gridpoint on the map. CellGraph::get_path, on lines 176-252, follows the actual A* algorithm in detail. The heuristic used for the purposes of this project is the absolute (RMS) distance - defined in lines 155-166.

For the gesture recognition, it was implemented by training the model with large amounts of input data We did three gestures and took over 400 photos for each gesture, the contents of which are visible in the github version history, they were deleted in the commit with the message “Reomve training data from repo”. The file “data_collect.py” in the “recognition” folder was used to take all these photos, standardizing the image size. After using this training data, we used teachable machine with google to generate the model. From there, it was a matter of properly loading the model and using it in real-time. This was done in the file “test.py” in the “recognition” folder. The main() function contains all of this, and contains comments guiding the steps of standardizing the hand image size in real-time (lines 43-69), as well as properly loading the model and using it. The real-time image was changed from a numpy array into an image and used with the model from lines 73-89. This could not be integrated with the other components due to the fact that tensorflow could not be used on the UTM or VM, so some code is commented out here, that was going to be used to integrate the components. On line 104, we intended to check if the same gesture was detected multiple frames in a row, so we knew it was that gesture. We intended to return this result and call all of this in the move_robot.py function, capturing the gesture recognized and using that to inform which position the robot should go to. We were going to code in two other positions that the robot could go to based on the gesture. That summarizes this portion of the project. This undertaking was helped by tutorials found online.
## ROS Node Diagram
![IMG_0009 (1)](https://user-images.githubusercontent.com/55162345/206756675-2c77e6d5-2105-4454-abb5-727acfaf7b31.jpg)
## Execution
Execution
To run the A* algorithm and movement, the robot must be placed in the known start position, which is also shown on the map. Once the robot is placed, this command can be run: “rosrun robotics_final_project move_robot.py”
To run the gesture recognition, the following installations need to be completed (this will not work on an m1 mac):
pip install tensorflow
pip install cvzone
pip install mediapipe
pip install opencv-python
After installing these, navigate to the “recognition” folder and run the following: “python3 test.py). After waiting a little bit for it to load, wave a hand in front of your computer’s camera and it will begin to run, a python script will be visibly running- this can be clicked on to see the recognition, as well as looking at the terminal for the results.
## Challenges, Future Work, and Takeaways
### Challenges
Our initial project idea was to have 2 robots (Tom and Jerry) communicate with each other and use A* to get to a specified destination, coordinate with each other how to pick up an object, and then maneuver back to another set location on the map using A*. We faced significant challenges from the very onset. Our first major challenge was to figure out how to maneuver two robots on the same teleop, from the same machine. We wanted to run particle filter on each robot individually, but on the same RVIZ map. We wanted each robot to be able to keep track of the other robot's relative position at all times, so that if the robots encountered each other in the maze, that they ignore each other and not mistake each other to be obstacles. This way, they'd still have a good idea for where they are on the map without having the presence of the other robot disrupt their estimated position. We ran into multiple roadblocks here. First, we had to figure out how to duplicate the particle_filter code in a way that each robot could have its own set of particles that would converge for its own robot. We experimented a lot with which topics they should both be subscribing to, and which topics they needed to subscribe to individually. We got stuck at the very last step, when we managed to have 2 sets of particle clouds for each robot, and one set of particles successfully converged for one robot, but the second set of particles would always think that it is also in the first robot's position. We diagnosed the issue and realized it had to do with both robots subscribing to the same "base_footprint" topic, which was actually a topic that we could not duplicate without editing the turtlebot3 documentation itself. We had spent over a week and a half trying to get this part of the project to work, since it was one of the major components of the project, and we felt extremely defeated when we had to start over from this point. 

So we decided to redefine our project using the components we had successfully completed (A* algorithm), and taking our newfound knowledge of particle filter. We decided instead to try and run particle filter on both robots from separate machines instead. However, we realized that it would be extremely difficult to have them communicate their positions with each other, and that particle filter was beginning to feel more and more unreliable as a method to determine the robots' relative poses in the map. We decided to evolve our project further, and just have 1 robot do A*, while another robot just traveled around the map, and the A* robot would have to manuever around the other robot whenever it encountered it and recompute its A* path to keep going. At this point, we were already 2 weeks into the project and had 1 week left for the final project. This brings us to our next series of challenges that we faced, attempting to get the robot to travel A*.

After computing the A* path and plotting its corresponding points on RVIZ, we also had to translate the movements on the RVIZ map to actual movements that a physical robot would have to take in the maze. At first, we tried to use particle_filter again to keep track of the robot's estimated pose in the maze, and use the robot's current estimated pose to course correct until it was on the computed A* points. The particle_filter ended up being way too unreliable to maintain a precise-enough estimate of the robot's position in the maze so that it could course-correct to the actual A* path. Every attempt we made to improve our particle filter algorithm was thwarted by other physical and hardware barriers -- like the robot's sensors overheating and becoming more unreliable as we kept testing, and that particle filter was at the end of the day, just providing an estimate of the robot's actual position. The robot would keep bumping into walls and fail to turn enough because the estimate was not precise enough. Next, we tried to use the built-in particle filter system, from the move_base topic. We worked with a couple other groups who also were attempting to do A* with move_base, but for some reason even after implementing move_base, the physical turtlebots would never actually move forward. We tampered with this for a while before realizing it might be better to try an alternative. We then decided to try using the turtlebot's odometry, and compute the total distance the robot would need to travel from point to point in the A* path that we generated. We ended up constructing a "reduced" version of our path to minimize the turns our turtlebot would need to take, we computed the change in yaw at each point that the turtlebot would have to turn, and then we also experimented with the start posiiton on the physical map to find the most accurate correspondence with the start position that we defined on the virtual map.

Finally, we also faced challenges with gesture recognition when it came to imports and machine compatibility. We couldn't get the gesture recognition output to successfully connect to our ROS scripts because the machine that had the gesture-recognition code on it did not have a VM to run the move_robot script, and all the computers that had the VM to run the move_robot scripts could not import some of the packages needed to run the gesture recognition script. 
### Future Work
Our short-term goal would be to connect the gesture recognition output to the robot movement script. 

Our long-term goals would include having some type of localization so robot can keep track of its current position in the map and to course correct. We would either have to siginificantly improve particle filter even more somehow or maybe get stronger sensor equipment or use a new type of localization method. We could also add another robot in the environment to have 2 robots doing A* at some point, but the robots would have to be able to communicate with each other somehow or recognize that the other robot is not an obstacle. Finally, taking these previous two goals, we could attempt our initial high-scope project idea, which was to have 2 robots using A* in a maze and coordinating to lift an object and maneuver it back out of the maze.
### Takeaways
* Start off with a smaller scope when you're planning a long-term project because it's easier to add components than to redesign a project to have a smaller scope.
* Don't rely on particle filter that we created in the prior project to keep track of the robot's estimated pose because it will not be reliable or precise enough.
* Gesture recognition stuff is very dependent on the type of machine and OS you have, which can make it very difficult to integrate with other things.
