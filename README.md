# Robotics Final

## Project Description
Based on various challenges we faced along the way, we had to repeatedly redefine our goals and the scope. In the end, our final project topic was to implement gesture recognition (computer vision algorithm) and an A* path following algorithm, which also included having the physical robot follow this path in the maze. This project idea was inspired by the idea of a robotic dog -- "turtledog" is an endearing term we've used to describe the final product -- which can recognize commands from a user that signal where it should travel, and it will travel to the destination using an optimal path. We can extend this project idea beyond the classroom by connecting it to a "search and retrieval" or even "search and rescue"-type situation. To accomplish our project task, we split our project into 3 primary components: 1) the A* algorithm, 2) the gesture recognition component, and 3) the robot movement itself, using Odometry to follow the A*-planned path. 
## System Architecture
## ROS Node Diagram
## Execution
## Challenges, Future Work, and Takeaways
### Challenges
Our initial project idea was to have 2 robots (Tom and Jerry) communicate with each other and use A* to get to a specified destination, coordinate with each other how to pick up an object, and then maneuver back to another set location on the map using A*. We faced significant challenges from the very onset. Our first major challenge was to figure out how to maneuver two robots on the same teleop, from the same machine. We wanted to run particle filter on each robot individually, but on the same RVIZ map. We wanted each robot to be able to keep track of the other robot's relative position at all times, so that if the robots encountered each other in the maze, that they ignore each other and not mistake each other to be obstacles. This way, they'd still have a good idea for where they are on the map without having the presence of the other robot disrupt their estimated position. We ran into multiple roadblocks here. First, we had to figure out how to duplicate the particle_filter code in a way that each robot could have its own set of particles that would converge for its own robot. We experimented a lot with which topics they should both be subscribing to, and which topics they needed to subscribe to individually. We got stuck at the very last step, when we managed to have 2 sets of particle clouds for each robot, and one set of particles successfully converged for one robot, but the second set of particles would always think that it is also in the first robot's position. We diagnosed the issue and realized it had to do with both robots subscribing to the same "base_footprint" topic, which was actually a topic that we could not duplicate without editing the turtlebot3 documentation itself. We had spent over a week and a half trying to get this part of the project to work, since it was one of the major components of the project, and we felt extremely defeated when we had to start over from this point. 

So we decided to redefine our project using the components we had successfully completed (A* algorithm), and taking our newfound knowledge of particle filter. We decided instead to try and run particle filter on both robots from separate machines instead. However, we realized that it would be extremely difficult to have them communicate their positions with each other, and that particle filter was beginning to feel more and more unreliable as a method to determine the robots' relative poses in the map. We decided to evolve our project further, and just have 1 robot do A*, while another robot just traveled around the map, and the A* robot would have to manuever around the other robot whenever it encountered it and recompute its A* path to keep going. At this point, we were already 2 weeks into the project and had 1 week left for the final project. This brings us to our next series of challenges that we faced, attempting to get the robot to travel A*.

After computing the A* path and plotting its corresponding points on RVIZ, we also had to translate the movements on the RVIZ map to actual movements that a physical robot would have to take in the maze. At first, we tried to use particle_filter again to keep track of the robot's estimated pose in the maze, and use the robot's current estimated pose to course correct until it was on the computed A* points. The particle_filter ended up being way too unreliable to maintain a precise-enough estimate of the robot's position in the maze so that it could course-correct to the actual A* path. Every attempt we made to improve our particle filter algorithm was thwarted by other physical and hardware barriers -- like the robot's sensors overheating and becoming more unreliable as we kept testing, and that particle filter was at the end of the day, just providing an estimate of the robot's actual position. The robot would keep bumping into walls and fail to turn enough because the estimate was not precise enough. Next, we tried to use the built-in particle filter system, from the move_base topic. We worked with a couple other groups who also were attempting to do A* with move_base, but for some reason even after implementing move_base, the physical turtlebots would never actually move forward. We tampered with this for a while before realizing it might be better to try an alternative. We then decided to try using the turtlebot's odometry, and compute the total distance the robot would need to travel from point to point in the A* path that we generated. We ended up constructing a "reduced" version of our path to minimize the turns our turtlebot would need to take, we computed the change in yaw at each point that the turtlebot would have to turn, and then we also experimented with the start posiiton on the physical map to find the most accurate correspondence with the start position that we defined on the virtual map.

Finally, we also faced challenges with gesture recognition when it came to imports and machine compatibility. We couldn't get the gesture recognition output to successfully connect to our ROS scripts because the machine that had the gesture-recognition code on it did not have a VM to run the move_robot script, and all the computers that had the VM to run the move_robot scripts could not import some of the packages needed to run the gesture recognition script. 
### Future WOrk
Our short-term goal would be to connect the gesture recognition output to the robot movement script. 

Our long-term goals would include having some type of localization so robot can keep track of its current position in the map and to course correct. We would either have to siginificantly improve particle filter even more somehow or maybe get stronger sensor equipment or use a new type of localization method. We could also add another robot in the environment to have 2 robots doing A* at some point, but the robots would have to be able to communicate with each other somehow or recognize that the other robot is not an obstacle. Finally, taking these previous two goals, we could attempt our initial high-scope project idea, which was to have 2 robots using A* in a maze and coordinating to lift an object and maneuver it back out of the maze.
### Takeaways
* Start off with a smaller scope when you're planning a long-term project because it's easier to add components than to redesign a project to have a smaller scope.
* Don't rely on particle filter that we created in the prior project to keep track of the robot's estimated pose because it will not be reliable or precise enough.
* Gesture recognition stuff is very dependent on the type of machine and OS you have, which can make it very difficult to integrate with other things.
